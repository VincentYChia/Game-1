# Project Structure - Few-Shot LLM Training System
**Date**: 2026-01-14
**Status**: Production Ready

---

## Overview

This is a complete few-shot learning system for generating game content (items, skills, enemies, etc.) using Claude with training data. The system features:

- âœ… 15 generation systems (10 content + 5 placement)
- âœ… Data-driven validation (ranges, tags, enums)
- âœ… Auto-generated enhanced prompts
- âœ… One prompt file per system
- âœ… Complete test suite
- âœ… Manual tuning ready

---

## Directory Structure

```
Fewshot_llm/
â”‚
â”œâ”€â”€ ğŸ“„ run.py                        # â­ MAIN ENTRY POINT
â”‚   â””â”€â”€ Interactive menu to generate content
â”‚
â”œâ”€â”€ ğŸ“ prompts/                      # â­ PROMPT FILES (EDIT HERE)
â”‚   â”œâ”€â”€ system_prompts/              # Complete prompts LLM uses
â”‚   â”‚   â”œâ”€â”€ system_1.txt             # Smithing items
â”‚   â”‚   â”œâ”€â”€ system_2.txt             # Refining materials
â”‚   â”‚   â”œâ”€â”€ system_3.txt             # Alchemy potions
â”‚   â”‚   â”œâ”€â”€ system_4.txt             # Engineering devices
â”‚   â”‚   â”œâ”€â”€ system_5.txt             # Enchantments
â”‚   â”‚   â”œâ”€â”€ system_6.txt             # Enemies
â”‚   â”‚   â”œâ”€â”€ system_7.txt             # Loot drops
â”‚   â”‚   â”œâ”€â”€ system_8.txt             # Resource nodes
â”‚   â”‚   â”œâ”€â”€ system_10.txt            # Skills
â”‚   â”‚   â”œâ”€â”€ system_11.txt            # Titles
â”‚   â”‚   â”œâ”€â”€ system_1x2.txt           # Smithing placement
â”‚   â”‚   â”œâ”€â”€ system_2x2.txt           # Refining placement
â”‚   â”‚   â”œâ”€â”€ system_3x2.txt           # Alchemy placement
â”‚   â”‚   â”œâ”€â”€ system_4x2.txt           # Engineering placement
â”‚   â”‚   â””â”€â”€ system_5x2.txt           # Enchanting placement
â”‚   â”œâ”€â”€ components/                  # Prompt components
â”‚   â”‚   â”œâ”€â”€ base/                    # Base prompts (human-written)
â”‚   â”‚   â””â”€â”€ enhanced/                # Auto-generated guidance
â”‚   â””â”€â”€ README.txt                   # Detailed prompt docs
â”‚
â”œâ”€â”€ ğŸ“ config/                       # Configuration files
â”‚   â”œâ”€â”€ system_metadata.json         # System names and templates
â”‚   â”œâ”€â”€ test_inputs.json             # Test inputs for each system
â”‚   â””â”€â”€ validation_libraries.json   # Auto-generated validation data
â”‚
â”œâ”€â”€ ğŸ“ examples/                     # Training data
â”‚   â””â”€â”€ few_shot_examples.json       # 83 examples across all systems
â”‚
â”œâ”€â”€ ğŸ“ src/                          # Source code
â”‚   â”œâ”€â”€ llm_runner.py                # LLM API integration
â”‚   â”œâ”€â”€ validator.py                 # Multi-layer validation
â”‚   â”œâ”€â”€ library_analyzer.py          # Extracts validation data
â”‚   â”œâ”€â”€ prompt_generator.py          # Generates enhanced prompts
â”‚   â”œâ”€â”€ update_system_prompts.py     # Combines base + enhanced
â”‚   â”œâ”€â”€ material_enricher.py         # Enriches inputs with material data
â”‚   â”œâ”€â”€ comprehensive_validation_test.py  # Test suite
â”‚   â”œâ”€â”€ visualize_placement.py       # ASCII placement visualizer
â”‚   â”œâ”€â”€ ui_visualizer.py             # Game UI visualizer (needs pygame)
â”‚   â””â”€â”€ refactor_prompts.py          # Prompt refactoring script
â”‚
â”œâ”€â”€ ğŸ“ outputs/                      # Generated outputs
â”‚   â”œâ”€â”€ validation_test_results.json # Latest validation results
â”‚   â””â”€â”€ (timestamped output files)
â”‚
â”œâ”€â”€ ğŸ“ archive/                      # Archived files
â”‚   â”œâ”€â”€ old_outputs/                 # Old test outputs
â”‚   â”œâ”€â”€ system_prompts.json.old      # Old monolithic prompts
â”‚   â””â”€â”€ (other archived files)
â”‚
â”œâ”€â”€ ğŸ“„ MANUAL_TUNING_GUIDE.md        # â­ HOW TO EDIT PROMPTS
â”œâ”€â”€ ğŸ“„ VALIDATION_ENHANCEMENT_SUMMARY.md  # Validation system docs
â”œâ”€â”€ ğŸ“„ PROMPT_REFACTORING_SUMMARY.md      # Prompt refactoring details
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md          # This file
â””â”€â”€ ğŸ“„ README.md                     # Project overview
```

---

## Key Files Quick Reference

### For Manual Tuning
| File | Purpose |
|------|---------|
| `MANUAL_TUNING_GUIDE.md` | **START HERE** - Complete guide to editing prompts |
| `prompts/system_prompts/*.txt` | Edit these files to change what LLM sees |
| `run.py` | Run this to test your changes |

### For Understanding the System
| File | Purpose |
|------|---------|
| `PROJECT_STRUCTURE.md` | This file - system overview |
| `VALIDATION_ENHANCEMENT_SUMMARY.md` | How validation works |
| `PROMPT_REFACTORING_SUMMARY.md` | How prompts are organized |
| `prompts/README.txt` | Detailed prompt system documentation |

### For Development
| File | Purpose |
|------|---------|
| `src/library_analyzer.py` | Re-analyze training data |
| `src/prompt_generator.py` | Regenerate enhanced prompts |
| `src/update_system_prompts.py` | Update final prompts |
| `src/validator.py` | Modify validation rules |

---

## Workflows

### ğŸ¯ Manual Prompt Tuning (Most Common)

1. **Edit prompt**: Open `prompts/system_prompts/system_X.txt`
2. **Test**: `python run.py` â†’ select system
3. **Check**: Look in `outputs/` directory
4. **Validate**: `python src/comprehensive_validation_test.py`
5. **Iterate**: Repeat until satisfied

**Time**: 5-15 minutes per iteration

### ğŸ”„ Update Enhanced Prompts (After Training Data Changes)

1. **Analyze**: `python src/library_analyzer.py`
2. **Generate**: `python src/prompt_generator.py`
3. **Update**: `python src/update_system_prompts.py`
4. **Test**: Run a few systems to verify

**Time**: 2-3 minutes

### âœ… Comprehensive Testing

1. **Generate**: `python run.py` â†’ option 1 (all systems)
2. **Validate**: `python src/comprehensive_validation_test.py`
3. **Review**: Check `outputs/validation_test_results.json`
4. **Fix**: Edit prompts for any issues
5. **Retest**: Repeat until clean

**Time**: 10-20 minutes

---

## System Mapping

### Content Generation Systems

| System | File | Template | Generates |
|--------|------|----------|-----------|
| 1 | `system_1.txt` | smithing_items | Weapons, armor, tools |
| 2 | `system_2.txt` | refining_items | Ingots, planks, materials |
| 3 | `system_3.txt` | alchemy_items | Potions, consumables |
| 4 | `system_4.txt` | engineering_items | Turrets, traps, bombs |
| 5 | `system_5.txt` | enchanting_recipes | Enchantments |
| 6 | `system_6.txt` | hostiles | Enemy definitions |
| 7 | `system_7.txt` | refining_items | Loot drops |
| 8 | `system_8.txt` | node_types | Resource nodes |
| 10 | `system_10.txt` | skills | Player skills |
| 11 | `system_11.txt` | titles | Achievement titles |

### Placement Systems

| System | File | Generates |
|--------|------|-----------|
| 1x2 | `system_1x2.txt` | Smithing grid placements |
| 2x2 | `system_2x2.txt` | Refining hub placements |
| 3x2 | `system_3x2.txt` | Alchemy sequence placements |
| 4x2 | `system_4x2.txt` | Engineering slot placements |
| 5x2 | `system_5x2.txt` | Enchanting pattern placements |

---

## Validation System

### Three-Layer Validation

1. **Structure Validation**
   - Checks JSON structure against template
   - Validates data types
   - Ensures required fields present

2. **Range Validation**
   - Checks numeric values against tier-based ranges
   - Â±33% tolerance allowed
   - Example: T1 damage 10-30, acceptable: 6.7-40

3. **Content Validation**
   - **Tags**: Must exist in template-specific library
   - **Enums**: Must use known values (category, type, etc.)
   - Shows valid options when mismatch found

### Validation Libraries

Auto-generated from training data:
- **Stat ranges**: Min/max/mean/median by tier
- **Tag libraries**: Valid tags per template
- **Enum values**: Valid options for constrained fields

**File**: `config/validation_libraries.json`

---

## Prompt System

### How Prompts Work

Each system has **one complete prompt file**:
```
prompts/system_prompts/system_1.txt
```

This file contains:
1. **Base prompt** (human-written)
   - Core task description
   - Basic instructions
2. **Enhanced guidance** (auto-generated)
   - Field-by-field guidance
   - Stat ranges by tier
   - Valid tag lists
   - Enum options

### Prompt Generation Pipeline

```
Training Data
    â†“
library_analyzer.py â†’ validation_libraries.json
    â†“
prompt_generator.py â†’ prompts/enhanced/*.txt
    â†“
update_system_prompts.py â†’ prompts/system_prompts/*.txt
    â†“
run.py â†’ Uses final prompts
```

### Where to Edit

| Goal | Edit This | Then Run |
|------|-----------|----------|
| Quick tweaks | `prompts/system_prompts/system_X.txt` | `python run.py` |
| Change base instructions | `prompts/components/base/system_X_base.txt` | `python src/update_system_prompts.py` |
| Update after training data change | N/A | Full pipeline (3 commands) |

---

## Statistics

### Training Data
- **83 total examples** across all systems
- **35 smithing items** (most complete)
- **25 hostiles** (enemies)
- **30 skills**
- **10 titles**

### Validation Libraries
- **118 stat ranges** extracted
- **269 metadata tags** identified
- **38 enum fields** detected
- **9 templates** analyzed

### Code
- **~2,500 lines** of source code
- **10 Python modules** in src/
- **15 system prompts** (one per system)
- **81% size reduction** vs old system

---

## Recent Changes

### 2026-01-14: Major Refactoring

1. **Prompt Consolidation**
   - Eliminated 81% duplication
   - One file per system
   - Changed .md â†’ .txt for clarity

2. **Validation Enhancement**
   - Added range checking (Â±33% tolerance)
   - Added tag validation
   - Added enum validation
   - Improved enum detection

3. **System Organization**
   - Created components structure
   - Separated base from enhanced prompts
   - Archived old files
   - Created comprehensive docs

### Commits
- `REFACTOR: Consolidate system prompts` (51 files)
- `CHANGE: Rename .md to .txt` (44 files)
- `FEAT: Enhanced validator + Library-driven prompts` (11 files)

---

## Common Tasks

### Generate Content for One System
```bash
python run.py
# Select: 2 (Run a SINGLE system)
# Enter: 1 (for system 1, smithing)
# Output: outputs/system_1_TIMESTAMP.json
```

### Test All Systems
```bash
python run.py
# Select: 1 (Run ALL systems)
# Wait for completion
# Check: outputs/ directory
```

### Validate Outputs
```bash
python src/comprehensive_validation_test.py
# Results: outputs/validation_test_results.json
```

### Update Prompts
```bash
# After editing base prompts:
python src/update_system_prompts.py

# After training data changes:
python src/library_analyzer.py
python src/prompt_generator.py
python src/update_system_prompts.py
```

### Visualize Placements
```bash
python src/visualize_placement.py
# Shows ASCII visualization of placement outputs
```

---

## Dependencies

### Python Packages
- `anthropic` - Claude API
- `json`, `pathlib` - Built-in
- `dataclasses` - Built-in

### Optional
- `pygame` - For UI visualizer (not required)

### External
- Game codebase (`Game-1-modular`) - For material database lookups

---

## API Configuration

Edit `run.py` to change:
```python
API_KEY = "sk-ant-api03-..."  # Your API key
MODEL = "claude-sonnet-4-20250514"  # Model to use
MAX_TOKENS = 2000
TEMPERATURE = 1.0
TOP_P = 0.999
```

---

## Troubleshooting

### "No such file or directory"
- Run from: `Fewshot_llm/` directory
- Command: `python run.py` (not `python src/run.py`)

### "Template not found"
- Check: `config/system_metadata.json`
- Verify: Template names match directories

### "Validation always fails"
- Check: `Â±33%` tolerance exists for ranges
- Update: Re-run `library_analyzer.py` if training data changed
- Review: `outputs/validation_test_results.json` for details

### "Outputs are empty"
- Check: API key is valid
- Check: Model name is correct
- Check: Test inputs exist in `config/test_inputs.json`

---

## Next Steps

1. âœ… **Read**: `MANUAL_TUNING_GUIDE.md`
2. âœ… **Open**: `prompts/system_prompts/system_1.txt`
3. âœ… **Run**: `python run.py` â†’ select system 1
4. âœ… **Check**: `outputs/` directory
5. âœ… **Edit**: Prompts based on results
6. âœ… **Iterate**: Repeat until satisfied

---

**System is ready for manual tuning! ğŸš€**
